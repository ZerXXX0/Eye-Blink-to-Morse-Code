{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 30.7/42.2 kB 445.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.2/42.2 kB 409.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (21.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (6.0.2)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.5)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "     ---------------------------------------- 0.0/76.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.3/76.3 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zerx\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "   ---------------------------------------- 0.0/494.8 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 368.6/494.8 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  491.5/494.8 kB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 494.8/494.8 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.6 MB 10.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.6 MB 9.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/11.6 MB 10.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.0/11.6 MB 10.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.5/11.6 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.2/11.6 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.6/11.6 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.0/11.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.9/11.6 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.4/11.6 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.8/11.6 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.1/11.6 MB 10.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.3/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.5/11.6 MB 9.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.4/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.9/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "   ---------------------------------------- 0.0/193.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 193.6/193.6 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 409.6/561.5 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 561.5/561.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 276.2/276.2 kB 16.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.6/2.7 MB 17.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.7/2.7 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.2/2.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
      "   ---------------------------------------- 0.0/453.3 kB ? eta -:--:--\n",
      "   --------------------------------------  450.6/453.3 kB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 453.3/453.3 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.7/86.7 kB ? eta 0:00:00\n",
      "Installing collected packages: xxhash, regex, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.5\n",
      "    Uninstalling huggingface-hub-0.26.5:\n",
      "      Successfully uninstalled huggingface-hub-0.26.5\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.34.4 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 regex-2025.9.1 tokenizers-0.22.0 transformers-4.56.1 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ZerX\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where your CSV files are stored\n",
    "path = \"NLP Dataset/\"\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "\n",
    "# Save to a single CSV\n",
    "df.to_csv(\"merged Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Pelajaran\", \"tipe_kesalahan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af20dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Example: load your dataframe\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Function to inject artificial misspacing\n",
    "def inject_misspace(sentence, prob=0.3):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if i < len(words) - 1 and random.random() < prob:\n",
    "            # Merge current word with the next one\n",
    "            new_words.append(words[i] + words[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "            i += 1\n",
    "\n",
    "    # Random split inside a word (less frequent)\n",
    "    for j in range(len(new_words)):\n",
    "        if len(new_words[j]) > 4 and random.random() < 0.1:\n",
    "            pos = random.randint(1, len(new_words[j]) - 2)\n",
    "            new_words[j] = new_words[j][:pos] + \" \" + new_words[j][pos:]\n",
    "    \n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# Create augmented data\n",
    "augmented = df[\"kalimat_salah\"].apply(inject_misspace)\n",
    "\n",
    "# Append to dataframe with label for clean vs augmented\n",
    "df_aug = df.copy()\n",
    "df_aug[\"kalimat_salah\"] = augmented\n",
    "df_aug[\"augmented\"] = True\n",
    "\n",
    "df[\"augmented\"] = False\n",
    "\n",
    "# Combine original + augmented\n",
    "df_final = pd.concat([df, df_aug], ignore_index=True)\n",
    "\n",
    "# Example: save\n",
    "# df_final.to_csv(\"dataset_with_misspaces.csv\", index=False)\n",
    "\n",
    "print(df_final.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18632db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns=[\"Pelajaran\", \"tipe_kesalahan\", \"augmented\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"df_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0828ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     AutoTokenizer,\n\u001b[0;32m      6\u001b[0m     EncoderDecoderModel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     DataCollatorForSeq2Seq\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:2866\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;66;03m# `_import_device_backends` should be kept at the end to ensure\u001b[39;00m\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;66;03m# all the other functions in this module that may be accessed by\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;66;03m# an autoloaded backend are defined\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_device_backend_autoload_enabled():\n\u001b[1;32m-> 2866\u001b[0m     \u001b[43m_import_device_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:2816\u001b[0m, in \u001b[0;36m_import_device_backends\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2814\u001b[0m     backend_extensions \u001b[38;5;241m=\u001b[39m entry_points()\u001b[38;5;241m.\u001b[39mget(group_name, ())\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2816\u001b[0m     backend_extensions \u001b[38;5;241m=\u001b[39m \u001b[43mentry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m backend_extension \u001b[38;5;129;01min\u001b[39;00m backend_extensions:\n\u001b[0;32m   2819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2820\u001b[0m         \u001b[38;5;66;03m# Load the extension\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:1041\u001b[0m, in \u001b[0;36mentry_points\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m:return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   1039\u001b[0m     dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1040\u001b[0m )\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSelectableGroups\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:476\u001b[0m, in \u001b[0;36mSelectableGroups.load\u001b[1;34m(cls, eps)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, eps):\n\u001b[0;32m    475\u001b[0m     by_group \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 476\u001b[0m     ordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(ordered, by_group)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((group, EntryPoints(eps)) \u001b[38;5;28;01mfor\u001b[39;00m group, eps \u001b[38;5;129;01min\u001b[39;00m grouped)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:1039\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[EntryPoints, SelectableGroups]:\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    :return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m     eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 1039\u001b[0m         \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1040\u001b[0m     )\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SelectableGroups\u001b[38;5;241m.\u001b[39mload(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:636\u001b[0m, in \u001b[0;36mDistribution.entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints\u001b[38;5;241m.\u001b[39m_from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentry_points.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\metadata\\__init__.py:939\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    938\u001b[0m     ):\n\u001b[1;32m--> 939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pathlib.py:1058\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 1. Load and Prepare Data\n",
    "# =========================\n",
    "# CSV must have \"kalimat_salah\" and \"kalimat_benar\"\n",
    "df = pd.read_csv(\"dataset_with_missspaces.csv\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# ---- Split into 70/20/10 ----\n",
    "dataset_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "temp_split = dataset_split[\"test\"].train_test_split(test_size=1/3, seed=42)\n",
    "\n",
    "dataset_final = {\n",
    "    \"train\": dataset_split[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"]\n",
    "}\n",
    "\n",
    "print(dataset_final)\n",
    "\n",
    "# =========================\n",
    "# 2. Tokenizer and Model\n",
    "# =========================\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Encoder-decoder setup\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n",
    "\n",
    "# =========================\n",
    "# 3. Preprocess Function\n",
    "# =========================\n",
    "max_length = 64\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"kalimat_salah\"], \n",
    "                       truncation=True, \n",
    "                       padding=\"max_length\", \n",
    "                       max_length=max_length)\n",
    "    outputs = tokenizer(batch[\"kalimat_benar\"], \n",
    "                        truncation=True, \n",
    "                        padding=\"max_length\", \n",
    "                        max_length=max_length)\n",
    "\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    tokenized_dataset[split] = dataset_final[split].map(preprocess, batched=True, remove_columns=df.columns)\n",
    "\n",
    "# =========================\n",
    "# 4. Training Setup\n",
    "# =========================\n",
    "batch_size = 16\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./indoBERT-corrector\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,          # keep only 2 best checkpoints\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True, # <-- saves best checkpoint\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5. Trainer\n",
    "# =========================\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6. Train Model\n",
    "# =========================\n",
    "trainer.train()\n",
    "\n",
    "# =========================\n",
    "# 7. Evaluate on Test Set\n",
    "# =========================\n",
    "results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Test results:\", results)\n",
    "\n",
    "# =========================\n",
    "# 8. Save Final Best Model\n",
    "# =========================\n",
    "trainer.save_model(\"./indoBERT-best-corrector\")\n",
    "tokenizer.save_pretrained(\"./indoBERT-best-corrector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25b337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
